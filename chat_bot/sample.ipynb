{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-medium-4k-instruct-bnb-4bit\")\n",
    "device = \"cuda:0\"\n",
    "# Load the model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/Phi-3-medium-4k-instruct-bnb-4bit\",\n",
    "    device_map=device,  # Automatically assign layers to GPU\n",
    "    load_in_4bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "def update_history(user_input, bot_response, history):\n",
    "    history.append(f\"User: {user_input}\")\n",
    "    history.append(f\"Chatbot: {bot_response}\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def generate_response(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "    return response\"\"\"\n",
    "\n",
    "\"\"\"import gradio as gr\n",
    "\n",
    "# Define the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_response,  # The function to call\n",
    "    inputs=gr.Textbox(lines=5, placeholder=\"Enter your prompt here...\"),  # Input box\n",
    "    outputs=gr.Textbox(lines=10, placeholder=\"The model's response will appear here...\"),  # Output box\n",
    "    title=\"Local LLM with Gradio\",\n",
    "    description=\"Ask anything to your locally hosted LLM powered by unsloth/Phi-3-medium-4k-instruct-bnb-4bit.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_history(prompt, history, max_history_length=5):\n",
    "    # Limit history to the last N exchanges\n",
    "    context = \"\\n\".join(history[-max_history_length * 2:])  # Two entries per exchange\n",
    "    full_prompt = f\"{context}\\nUser: {prompt}\\nChatbot:\"\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(user_input):\n",
    "    global history\n",
    "    \n",
    "    # Generate a response with history\n",
    "    bot_response = generate_response_with_history(user_input, history)\n",
    "    \n",
    "    # Update history\n",
    "    update_history(user_input, bot_response, history)\n",
    "    \n",
    "    return bot_response\n",
    "\n",
    "import gradio as gr\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=chatbot,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your message here...\"),\n",
    "    outputs=gr.Textbox(lines=5, placeholder=\"The bot's response will appear here...\"),\n",
    "    title=\"Chatbot with Memory\",\n",
    "    description=\"A chatbot that remembers recent conversation history.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuaccess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
